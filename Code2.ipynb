{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Credit: Codes were adopted from https://github.com/Sungwon-Han/urban_score\n",
        "\n",
        "Sungwon Han, Donghyun Ahn, Sungwon Park, Jeasurk Yang, Susang Lee, Jihee Kim, Hyunjoo Yang, Sangyoon Park, and Meeyoung Cha. 2020. Learning to Score Economic Development from Satellite Imagery. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD '20). Association for Computing Machinery, New York, NY, USA, 2970â€“2979. https://doi.org/10.1145/3394486.3403347"
      ],
      "metadata": {
        "id": "o_CPf_w7IMcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install finch-clust"
      ],
      "metadata": {
        "id": "RhqbyeokLvu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import preprocessing\n",
        "from finch import FINCH\n",
        "\n",
        "for target_index in ['index002','index004','index006','index008','index01']:\n",
        "    # Define the path to your data file\n",
        "    data_path_eviction = './AirbnbData/Chicago/train-gridsize-{}.csv'.format(str(target_index)[5:])\n",
        "\n",
        "    # Load the data using pandas\n",
        "    data_eviction_df = pd.read_csv(data_path_eviction)  # Adjust if your file has a different delimiter or encoding\n",
        "    data_eviction_df = data_eviction_df.drop(columns=[target_index, 'label'])\n",
        "    # Convert DataFrame to numpy array\n",
        "    data_eviction = data_eviction_df.values\n",
        "\n",
        "    # Handle missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    data_eviction = imputer.fit_transform(data_eviction)\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = preprocessing.MinMaxScaler()\n",
        "    data_eviction = scaler.fit_transform(data_eviction)\n",
        "\n",
        "    # Run FINCH clustering algorithm\n",
        "    c_eviction, num_clust_eviction, req_c_eviction = FINCH(data_eviction, distance='euclidean')\n",
        "\n",
        "    # Prepare data for CSV\n",
        "    # Create a DataFrame with original data, cluster labels, and partition information\n",
        "    cluster_df = pd.DataFrame(data_eviction_df)\n",
        "\n",
        "    # Add partition information\n",
        "    for i in range(c_eviction.shape[1]):\n",
        "        cluster_df[f'Partition_{i+1}'] = c_eviction[:, i]\n",
        "\n",
        "    # Define the path for the output CSV file\n",
        "    output_csv_path = './AirbnbData/Chicago/grid{}results.csv'.format(str(target_index)[5:])  # Get the output file path from command line argument\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    cluster_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "for target_index in ['index002','index004','index006','index008','index01']:\n",
        "            print(target_index)\n",
        "            train_data = pd.read_csv('./AirbnbData/Chicago/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n",
        "            cluster_labels = list(pd.read_csv('./AirbnbData/Chicago/grid{}results.csv'.format(str(target_index)[5:]))['Partition_1'].values)\n",
        "            result = pd.DataFrame({\"y_train\":list(train_data['label'].values), \"cluster_labels\":cluster_labels})\n",
        "            result.to_csv('./AirbnbData/Chicago/finch-train-gridsize-{}-c.csv'.format(str(target_index)[5:]))"
      ],
      "metadata": {
        "id": "9-9x09jXRRfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "for target_index in ['index002','index004','index006','index008','index01']:\n",
        "    census_geoid_df = pd.read_csv('./AirbnbData/Chicago/grid_population_mapping.csv')\n",
        "    census_geoid_df['GEOID'] = census_geoid_df['GEOID'].map(lambda x: str(x))\n",
        "    acs_df = pd.read_csv('./AirbnbData/Chicago/S2503Data.csv', skiprows = [1])[['S2503_C01_028E','S2503_C01_032E','S2503_C01_036E','S2503_C01_040E','S2503_C01_044E','GEO_ID']]\n",
        "    acs_df['CensusPop'] = acs_df['S2503_C01_028E'] + acs_df['S2503_C01_032E'] + acs_df['S2503_C01_036E'] + acs_df['S2503_C01_040E']+ acs_df['S2503_C01_044E']\n",
        "    acs_df['GEOID'] = acs_df['GEO_ID'].map(lambda x: str(x).replace(\"1400000US\",\"\"))\n",
        "    acs_df = acs_df[['GEOID','CensusPop']]\n",
        "    census_geoid_df = pd.merge(census_geoid_df, acs_df, on=\"GEOID\", how = \"left\")\n",
        "\n",
        "    train_data_class = pd.read_csv('./AirbnbData/Chicago/finch-train-gridsize-{}-c.csv'.format(str(target_index)[5:]))[['cluster_labels']]\n",
        "    cnum = len(list(set(list(train_data_class['cluster_labels'].values))))\n",
        "    print(target_index, cnum)\n",
        "    train_data = pd.read_csv('./AirbnbData/Chicago/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n",
        "    index_cluster_label_df = pd.concat([train_data, train_data_class], axis=1)\n",
        "\n",
        "    census_geoid_df = pd.merge(census_geoid_df, index_cluster_label_df, on=target_index, how = \"left\")\n",
        "    census_geoid_df = census_geoid_df.loc[census_geoid_df['cluster_labels'].notnull()]\n",
        "    final_data = census_geoid_df.groupby(['GEOID','CensusPop','cluster_labels']).agg(num_grids = (target_index,\"nunique\")).reset_index()\n",
        "    lst, lst2 = list(set(census_geoid_df['GEOID'].values)), list(range(cnum))\n",
        "    final_data = pd.merge(final_data, pd.DataFrame(list(itertools.product(lst, lst2)), columns=['GEOID','cluster_labels']), on=['GEOID', 'cluster_labels'], how=\"right\")\n",
        "    final_data.sort_values(by=['GEOID', 'cluster_labels'], inplace = True)\n",
        "    final_data = final_data.pivot(index=\"GEOID\", columns=\"cluster_labels\", values=\"num_grids\").reset_index()\n",
        "    final_data = pd.merge(final_data, acs_df, on=\"GEOID\", how=\"left\")\n",
        "    final_data = final_data.fillna(0)\n",
        "    final_data.columns = ['GEOID'] + ['label{}'.format(i) for i in range(cnum)]+ ['CensusPop']\n",
        "    final_data.to_csv('./AirbnbData/Chicago/finch-census-guided-targetindex{}-c.csv'.format(str(target_index)[5:]), index=False)\n"
      ],
      "metadata": {
        "id": "MF1Yz8-tSfrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from collections import defaultdict\n",
        "from functools import cmp_to_key\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self,vertices):\n",
        "        self.V= vertices\n",
        "        self.graph = defaultdict(list)\n",
        "\n",
        "    def addEdge(self,u,v):\n",
        "        self.graph[u].append(v)\n",
        "\n",
        "    def printPathsFunc(self, u, d, visited, path, current_path_list):\n",
        "        visited[u]= True\n",
        "        path.append(u)\n",
        "\n",
        "        if u == d:\n",
        "            path_copy = path[:]\n",
        "            current_path_list.append(path_copy)\n",
        "        else:\n",
        "            for i in self.graph[u]:\n",
        "                if visited[i]==False:\n",
        "                    self.printPathsFunc(i, d, visited, path, current_path_list)\n",
        "\n",
        "        path.pop()\n",
        "        visited[u]= False\n",
        "        return current_path_list\n",
        "\n",
        "\n",
        "    def printPaths(self, s, d):\n",
        "        total_results = []\n",
        "        for start in s:\n",
        "            for dest in d:\n",
        "                path = []\n",
        "                visited =[False]*(self.V)\n",
        "                current_path_list = []\n",
        "                current_path_results = self.printPathsFunc(start, dest, visited, path, current_path_list)\n",
        "                if len(current_path_results) != 0:\n",
        "                    total_results.extend(current_path_results)\n",
        "        return total_results\n",
        "\n",
        "\n",
        "def graph_process(config_path):\n",
        "    cluster_unify = []\n",
        "    partial_order = []\n",
        "    start_candidates = []\n",
        "    end_candidates = []\n",
        "\n",
        "    f = open(config_path, 'r')\n",
        "    while True:\n",
        "        line = f.readline()\n",
        "        if '=' in line:\n",
        "            unify = list(map(int, line.split('=')))\n",
        "            cluster_unify.append(unify)\n",
        "        elif '<' in line:\n",
        "            order = list(map(int, line.split('<')))\n",
        "            partial_order.append(order)\n",
        "            start_candidates.append(order[0])\n",
        "            end_candidates.append(order[1])\n",
        "\n",
        "        if not line: break\n",
        "    f.close()\n",
        "\n",
        "    start = []\n",
        "    end = []\n",
        "    for element in start_candidates:\n",
        "        if element in end_candidates:\n",
        "            continue\n",
        "        start.append(element)\n",
        "\n",
        "    for element in end_candidates:\n",
        "        if element in start_candidates:\n",
        "            continue\n",
        "        end.append(element)\n",
        "\n",
        "    start = list(set(start))\n",
        "    end = list(set(end))\n",
        "    return start, end, partial_order, cluster_unify\n",
        "\n",
        "\n",
        "\n",
        "def generate_graph(partial_order_list, vertex_num):\n",
        "    cluster_graph = Graph(vertex_num)\n",
        "    for pair in partial_order_list:\n",
        "        cluster_graph.addEdge(pair[0], pair[1])\n",
        "    return cluster_graph\n",
        "\n",
        "\n",
        "def save_graph_config(ordered_list, name):\n",
        "    f = open(\"./AirbnbData/Chicago/{}\".format(name), 'w+')\n",
        "\n",
        "    for i in range(len(ordered_list) - 1):\n",
        "        f.write('{}<{}\\n'.format(ordered_list[i+1][0], ordered_list[i][0]))\n",
        "\n",
        "    for orders in ordered_list:\n",
        "        if len(orders) >= 2:\n",
        "            f.write(str(orders[0]))\n",
        "            for element in orders[1:]:\n",
        "                f.write('={}'.format(element))\n",
        "            f.write('\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "def graph_inference_census(df, cluster_num, file_path):\n",
        "    def numeric_compare(x, y):\n",
        "        pop_list1 = result_list[x]\n",
        "        pop_list2 = result_list[y]\n",
        "        tTestResult = stats.ttest_ind(pop_list1, pop_list2)\n",
        "        if (tTestResult.pvalue < 0.05) and (np.mean(pop_list1) < np.mean(pop_list2)):\n",
        "            return 1\n",
        "        elif (tTestResult.pvalue < 0.05) and (np.mean(pop_list1) >= np.mean(pop_list2)):\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    result_list = []\n",
        "\n",
        "    for i in range(cluster_num):\n",
        "        result_list.append([])\n",
        "\n",
        "    for _ in range(100):\n",
        "        msk = np.random.rand(len(df)) < 0.5\n",
        "        df_train = df[msk][(['label{}'.format(i) for i in range(cluster_num)]+['CensusPop'])]\n",
        "        df_test = df[~msk][(['label{}'.format(i) for i in range(cluster_num)]+['CensusPop'])]\n",
        "\n",
        "        # selected_hist_train = hist.loc[df_train['Directory'] - 1]\n",
        "        # selected_hist_test = hist.loc[df_test['Directory'] - 1]\n",
        "\n",
        "        train_X = df_train[['label{}'.format(i) for i in range(cluster_num)]].values\n",
        "        train_y = df_train[\"CensusPop\"].values\n",
        "        test_X = df_train[['label{}'.format(i) for i in range(cluster_num)]].values\n",
        "        test_y = df_test[\"CensusPop\"].values\n",
        "        result = nnls(train_X, train_y, 100)[0]\n",
        "        for i in range(len(result)):\n",
        "            result_list[i].append(result[i])\n",
        "\n",
        "    sorted_list = sorted(range(cluster_num ), key=cmp_to_key(numeric_compare))\n",
        "    ordered_list = []\n",
        "    ordered_list.append([sorted_list[0]])\n",
        "    curr = 0\n",
        "    for i in range(len(sorted_list) - 1):\n",
        "        if numeric_compare(sorted_list[i], sorted_list[i+1]) == 0:\n",
        "            ordered_list[curr].append(sorted_list[i+1])\n",
        "        else:\n",
        "            curr += 1\n",
        "            ordered_list.append([sorted_list[i+1]])\n",
        "\n",
        "    # ordered_list.append([cluster_num])\n",
        "    save_graph_config(ordered_list, file_path)\n",
        "    return './graph_data/{}'.format(file_path)\n",
        "\n",
        "for target_index in ['index002','index004','index006','index008','index01']:\n",
        "        print(target_index)\n",
        "        df = pd.read_csv('./AirbnbData/Chicago/finch-census-guided-targetindex{}-c.csv'.format(str(target_index)[5:]))\n",
        "        graph_inference_census(df, cnum, \"finch-graph-config-target{}-c.txt\".format(target_index))\n"
      ],
      "metadata": {
        "id": "VGnlhR9rjOqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from collections import defaultdict\n",
        "from functools import cmp_to_key\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self,vertices):\n",
        "        self.V= vertices\n",
        "        self.graph = defaultdict(list)\n",
        "\n",
        "    def addEdge(self,u,v):\n",
        "        self.graph[u].append(v)\n",
        "\n",
        "    def printPathsFunc(self, u, d, visited, path, current_path_list):\n",
        "        visited[u]= True\n",
        "        path.append(u)\n",
        "\n",
        "        if u == d:\n",
        "            path_copy = path[:]\n",
        "            current_path_list.append(path_copy)\n",
        "        else:\n",
        "            for i in self.graph[u]:\n",
        "                if visited[i]==False:\n",
        "                    self.printPathsFunc(i, d, visited, path, current_path_list)\n",
        "\n",
        "        path.pop()\n",
        "        visited[u]= False\n",
        "        return current_path_list\n",
        "\n",
        "\n",
        "    def printPaths(self, s, d):\n",
        "        total_results = []\n",
        "        for start in s:\n",
        "            for dest in d:\n",
        "                path = []\n",
        "                visited =[False]*(self.V)\n",
        "                current_path_list = []\n",
        "                current_path_results = self.printPathsFunc(start, dest, visited, path, current_path_list)\n",
        "                if len(current_path_results) != 0:\n",
        "                    total_results.extend(current_path_results)\n",
        "        return total_results\n",
        "\n",
        "\n",
        "def graph_process(config_path):\n",
        "    cluster_unify = []\n",
        "    partial_order = []\n",
        "    start_candidates = []\n",
        "    end_candidates = []\n",
        "\n",
        "    f = open(config_path, 'r')\n",
        "    while True:\n",
        "        line = f.readline()\n",
        "        if '=' in line:\n",
        "            unify = list(map(int, line.split('=')))\n",
        "            cluster_unify.append(unify)\n",
        "        elif '<' in line:\n",
        "            order = list(map(int, line.split('<')))\n",
        "            partial_order.append(order)\n",
        "            start_candidates.append(order[0])\n",
        "            end_candidates.append(order[1])\n",
        "\n",
        "        if not line: break\n",
        "    f.close()\n",
        "\n",
        "    start = []\n",
        "    end = []\n",
        "    for element in start_candidates:\n",
        "        if element in end_candidates:\n",
        "            continue\n",
        "        start.append(element)\n",
        "\n",
        "    for element in end_candidates:\n",
        "        if element in start_candidates:\n",
        "            continue\n",
        "        end.append(element)\n",
        "\n",
        "    start = list(set(start))\n",
        "    end = list(set(end))\n",
        "    return start, end, partial_order, cluster_unify\n",
        "\n",
        "\n",
        "\n",
        "def generate_graph(partial_order_list, vertex_num):\n",
        "    cluster_graph = Graph(vertex_num)\n",
        "    for pair in partial_order_list:\n",
        "        cluster_graph.addEdge(pair[0], pair[1])\n",
        "    return cluster_graph\n",
        "\n",
        "\n",
        "def save_graph_config(ordered_list, name):\n",
        "    f = open(\"./AirbnbData/Chicago/{}\".format(name), 'w+')\n",
        "\n",
        "    for i in range(len(ordered_list) - 1):\n",
        "        f.write('{}<{}\\n'.format(ordered_list[i+1][0], ordered_list[i][0]))\n",
        "\n",
        "    for orders in ordered_list:\n",
        "        if len(orders) >= 2:\n",
        "            f.write(str(orders[0]))\n",
        "            for element in orders[1:]:\n",
        "                f.write('={}'.format(element))\n",
        "            f.write('\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "def graph_inference_census(df, cluster_num, file_path):\n",
        "    def numeric_compare(x, y):\n",
        "        pop_list1 = result_list[x]\n",
        "        pop_list2 = result_list[y]\n",
        "        tTestResult = stats.ttest_ind(pop_list1, pop_list2)\n",
        "        if (tTestResult.pvalue < 0.05) and (np.mean(pop_list1) < np.mean(pop_list2)):\n",
        "            return 1\n",
        "        elif (tTestResult.pvalue < 0.05) and (np.mean(pop_list1) >= np.mean(pop_list2)):\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    result_list = []\n",
        "\n",
        "    for i in range(cluster_num):\n",
        "        result_list.append([])\n",
        "\n",
        "    for _ in range(100):\n",
        "        msk = np.random.rand(len(df)) < 0.5\n",
        "        df_train = df[msk][(['label{}'.format(i) for i in range(cluster_num)]+['CensusPop'])]\n",
        "        df_test = df[~msk][(['label{}'.format(i) for i in range(cluster_num)]+['CensusPop'])]\n",
        "\n",
        "        # selected_hist_train = hist.loc[df_train['Directory'] - 1]\n",
        "        # selected_hist_test = hist.loc[df_test['Directory'] - 1]\n",
        "\n",
        "        train_X = df_train[['label{}'.format(i) for i in range(cluster_num)]].values\n",
        "        train_y = df_train[\"CensusPop\"].values\n",
        "        test_X = df_train[['label{}'.format(i) for i in range(cluster_num)]].values\n",
        "        test_y = df_test[\"CensusPop\"].values\n",
        "        result = nnls(train_X, train_y, 100)[0]\n",
        "        for i in range(len(result)):\n",
        "            result_list[i].append(result[i])\n",
        "\n",
        "    sorted_list = sorted(range(cluster_num ), key=cmp_to_key(numeric_compare))\n",
        "    ordered_list = []\n",
        "    ordered_list.append([sorted_list[0]])\n",
        "    curr = 0\n",
        "    for i in range(len(sorted_list) - 1):\n",
        "        if numeric_compare(sorted_list[i], sorted_list[i+1]) == 0:\n",
        "            ordered_list[curr].append(sorted_list[i+1])\n",
        "        else:\n",
        "            curr += 1\n",
        "            ordered_list.append([sorted_list[i+1]])\n",
        "\n",
        "    # ordered_list.append([cluster_num])\n",
        "    save_graph_config(ordered_list, file_path)\n",
        "    return './AirbnbData/Chicago/{}'.format(file_path)\n"
      ],
      "metadata": {
        "id": "4ORQozN47vKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from numpy import nanmin, nanmax, nanmean, nanvar, nanmedian\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, target_index, cnum):\n",
        "        test_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))\n",
        "        train_data = pd.read_csv('./AirbnbData/Chicago/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n",
        "        test_data.drop(['label', target_index], axis=1, inplace=True)\n",
        "        train_data.drop(['label', target_index], axis=1, inplace=True)\n",
        "\n",
        "        X_train =  train_data.values\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        imputer_model = imputer.fit(X_train)\n",
        "        X_train = imputer_model.transform(X_train)\n",
        "\n",
        "        X_test = imputer_model.transform(test_data.values)\n",
        "\n",
        "        # min-max scaling\n",
        "        min_max_scaler_model = (preprocessing.MinMaxScaler()).fit(X_train)\n",
        "        X_train = min_max_scaler_model.transform(X_train)\n",
        "        X_test = min_max_scaler_model.transform(X_test)\n",
        "\n",
        "        self.file_list = X_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.file_list).shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.file_list[idx], idx\n",
        "\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "class ClusterDataset(Dataset):\n",
        "    def __init__(self, cluster_list, target_index, cnum, batch_sz):\n",
        "        self.data_list = None\n",
        "        train_data_class = pd.read_csv('./AirbnbData/Chicago/finch-train-gridsize-{}-c.csv'.format(str(target_index)[5:]))[['cluster_labels']]\n",
        "        train_data_class['cluster_labels'] = train_data_class['cluster_labels'].map(lambda x: int(x))\n",
        "        train_data = pd.read_csv('./AirbnbData/Chicago/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n",
        "        train_data.drop(['label', target_index], axis=1, inplace=True)\n",
        "\n",
        "        X_train =  train_data.values\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        imputer_model = imputer.fit(X_train)\n",
        "        X_train = imputer_model.transform(X_train)\n",
        "        # min-max scaling\n",
        "        min_max_scaler_model = (preprocessing.MinMaxScaler()).fit(X_train)\n",
        "        X_train = min_max_scaler_model.transform(X_train)\n",
        "        train_data_v2 = pd.DataFrame(X_train, columns = train_data.columns)\n",
        "\n",
        "        for cluster_num in cluster_list:\n",
        "            tmp =train_data_v2[train_data_class['cluster_labels']==int(cluster_num)]\n",
        "            if self.data_list is None:\n",
        "                self.data_list = tmp.values\n",
        "            else:\n",
        "                self.data_list = np.concatenate([self.data_list, tmp.values])\n",
        "\n",
        "        if (self.data_list.shape[0]<batch_sz) and (self.data_list.shape[0] > 0):\n",
        "              self.data_list = (resample(pd.DataFrame(self.data_list), n_samples=batch_sz,replace=True)).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.data_list).shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data_list[idx]\n"
      ],
      "metadata": {
        "id": "lKDEpxgBdv8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "from itertools import permutations\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(181, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 16),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(16, 8),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(8, 1),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "    self.layers.cuda()\n",
        "    for m in self.layers.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "          m.weight.data = m.weight.data.double()\n",
        "          m.bias.data = m.bias.data.double()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "def make_data_loader(cluster_list, batch_sz, target_index, cluster_num):\n",
        "    cluster_dataset = ClusterDataset(cluster_list, target_index, cluster_num, batch_sz)\n",
        "    if cluster_dataset.__len__() == batch_sz:\n",
        "        cluster_loader = torch.utils.data.DataLoader(cluster_dataset, batch_size=batch_sz, shuffle=True, num_workers=4, drop_last=False)\n",
        "    else:\n",
        "        cluster_loader = torch.utils.data.DataLoader(cluster_dataset, batch_size=batch_sz, shuffle=True, num_workers=4, drop_last=True)\n",
        "    return cluster_loader\n",
        "\n",
        "\n",
        "def generate_loader_dict(total_list, unified_cluster_list, batch_sz, target_index, cluster_num):\n",
        "    loader_dict = {}\n",
        "    for cluster_id in total_list:\n",
        "        cluster_loader = make_data_loader([cluster_id], batch_sz, target_index, cluster_num)\n",
        "        loader_dict[cluster_id] = cluster_loader\n",
        "\n",
        "    for cluster_tuple in unified_cluster_list:\n",
        "        cluster_loader = make_data_loader(cluster_tuple, batch_sz, target_index, cluster_num)\n",
        "        for cluster_id in cluster_tuple:\n",
        "            loader_dict[cluster_id] = cluster_loader\n",
        "    return loader_dict\n",
        "\n",
        "\n",
        "def train(epoch, model, optimizer, loader_list, cluster_path_list, device, lr, batchsz, seed, lamb, alpha):\n",
        "    model.train()\n",
        "    train_loss = AverageMeter()\n",
        "    reg_loss = AverageMeter()\n",
        "\n",
        "    # For each cluster route\n",
        "    path_idx = 0\n",
        "    avg_loss = 0\n",
        "    count = 0\n",
        "    for cluster_path in cluster_path_list:\n",
        "        path_idx += 1\n",
        "        dataloaders = []\n",
        "        for cluster_id in cluster_path:\n",
        "            dataloaders.append(loader_list[cluster_id])\n",
        "\n",
        "        for batch_idx, data in enumerate(zip(*dataloaders)):\n",
        "            cluster_num = len(data)\n",
        "            data_zip = torch.cat(data, 0).to(device)\n",
        "\n",
        "            # Generating Score\n",
        "            scores = model(data_zip).squeeze()\n",
        "            scores = torch.clamp(scores, min=0, max=1)\n",
        "            score_list = torch.split(scores, batchsz, dim = 0)\n",
        "\n",
        "            # Standard deviation as a loss\n",
        "            loss_var = torch.zeros(1).to(device)\n",
        "            for score in score_list:\n",
        "                loss_var += score.var()\n",
        "            loss_var /= len(score_list)\n",
        "\n",
        "            # Differentiable Ranking with sigmoid function\n",
        "            rank_matrix = torch.zeros((batchsz, cluster_num, cluster_num)).to(device)\n",
        "            for itertuple in list(permutations(range(cluster_num), 2)):\n",
        "                score1 = score_list[itertuple[0]]\n",
        "                score2 = score_list[itertuple[1]]\n",
        "                diff = lamb * (score2 - score1)\n",
        "                results = torch.sigmoid(diff)\n",
        "                rank_matrix[:, itertuple[0], itertuple[1]] = results\n",
        "                rank_matrix[:, itertuple[1], itertuple[0]] = 1 - results\n",
        "\n",
        "            rank_predicts = rank_matrix.sum(1)\n",
        "            temp = torch.Tensor(range(cluster_num))\n",
        "            target_rank = temp.unsqueeze(0).repeat(batchsz, 1).to(device)\n",
        "\n",
        "            # Equivalent to spearman rank correlation loss\n",
        "            loss_train = ((rank_predicts - target_rank)**2).mean()\n",
        "            loss = loss_train + loss_var * alpha\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.update(loss_train.item(), batchsz)\n",
        "            reg_loss.update(loss_var.item(), batchsz)\n",
        "            avg_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "\n",
        "            # Print status\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Epoch: [{epoch}][{path_idx}][{elps_iters}] '\n",
        "                      'Train loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '\n",
        "                      'Reg loss: {reg_loss.val:.4f} ({reg_loss.avg:.4f})'.format(\n",
        "                          epoch=epoch, path_idx=path_idx, elps_iters=batch_idx, train_loss=train_loss, reg_loss=reg_loss))\n",
        "\n",
        "    return avg_loss / count\n",
        "\n",
        "\n",
        "def main(lr, batchsz, epochs, seed, lamb, alpha, cluster_num, target_index, modelname):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    graph_config = \"./AirbnbData/Chicago/finch-graph-config-target{}-c.txt\".format(target_index)\n",
        "    start, end, partial_order, cluster_unify = graph_process(graph_config)\n",
        "\n",
        "    loader_list = generate_loader_dict(range(cluster_num), cluster_unify, batchsz, target_index, cluster_num)\n",
        "    cluster_graph = generate_graph(partial_order, cluster_num)\n",
        "    cluster_path_list = cluster_graph.printPaths(start, end)\n",
        "    print(\"Cluster_path: \", cluster_path_list)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = MLP()\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "    valid_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))[['label']]\n",
        "\n",
        "    best_spearman_corr = -2\n",
        "    patience = 0\n",
        "    for epoch in range(epochs):\n",
        "            loss = train(epoch, model, optimizer, loader_list, cluster_path_list, device, lr, batchsz, seed, lamb, alpha)\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            test_dataset = TestDataset(target_index, cluster_num)\n",
        "            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batchsz, shuffle=False, num_workers=4)\n",
        "\n",
        "            prediction_list = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (data, name) in enumerate(test_loader):\n",
        "                    data = data.to(device)\n",
        "                    scores = model(data).squeeze()\n",
        "                    count = 0\n",
        "                    for each_name in name:\n",
        "                        prediction_list.append(scores[count].cpu().data.numpy())\n",
        "                        count += 1\n",
        "\n",
        "            spearman_corr = spearmanr(np.array(prediction_list), valid_data.values).statistic\n",
        "            print(spearman_corr)\n",
        "\n",
        "            if best_spearman_corr < spearman_corr:\n",
        "                    patience = 0\n",
        "                    print(\"state saving...\")\n",
        "                    state = {\n",
        "                        'model': model.state_dict(),\n",
        "                        'loss': loss\n",
        "                    }\n",
        "                    if not os.path.isdir('./AirbnbData/Chicago/checkpoint'):\n",
        "                        os.mkdir('./AirbnbData/Chicago/checkpoint')\n",
        "                    torch.save(state, './AirbnbData/Chicago/checkpoint/{}'.format(modelname))\n",
        "                    best_spearman_corr = spearman_corr\n",
        "            else:\n",
        "                    patience += 1\n",
        "                    if patience > 10:\n",
        "                      break\n",
        "\n",
        "for target_index in ['index002','index004','index006','index008','index01']:\n",
        "      dummy = pd.read_csv('./AirbnbData/Chicago/finch-train-gridsize-{}-c.csv'.format(str(target_index)[5:]))[['cluster_labels']]\n",
        "      cluster_num = len(list(set(list(train_data_class['cluster_labels'].values))))\n",
        "      print(target_index, cluster_num)\n",
        "      main(lr = 0.0002, batchsz=32, epochs=1000, seed=1567010775, lamb=30, alpha=4, cluster_num=cluster_num, target_index=target_index, modelname = 'finch-checkpoint_target_{}_c.pth'.format(target_index))\n"
      ],
      "metadata": {
        "id": "jaGCVibVILTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from numpy import nanmin, nanmax, nanmean, nanvar, nanmedian\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "from itertools import permutations\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, target_index, cnum):\n",
        "        test_data = pd.read_csv('./NYC_Experiments/data/test-gridsize-{}.csv'.format(str(target_index)[5:]))\n",
        "        train_data = pd.read_csv('./AirbnbData/Chicago Airbnb 07:2023/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n",
        "        test_data.drop(['label', target_index], axis=1, inplace=True)\n",
        "        train_data.drop(['label', target_index], axis=1, inplace=True)\n",
        "\n",
        "        X_train =  train_data.values\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        imputer_model = imputer.fit(X_train)\n",
        "        X_train = imputer_model.transform(X_train)\n",
        "\n",
        "        X_test = imputer_model.transform(test_data.values)\n",
        "\n",
        "        # min-max scaling\n",
        "        min_max_scaler_model = (preprocessing.MinMaxScaler()).fit(X_train)\n",
        "        X_train = min_max_scaler_model.transform(X_train)\n",
        "        X_test = min_max_scaler_model.transform(X_test)\n",
        "\n",
        "        self.file_list = X_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.file_list).shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.file_list[idx], idx\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(181, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 16),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(16, 8),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(8, 1),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "    self.layers.cuda()\n",
        "    for m in self.layers.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "          m.weight.data = m.weight.data.double()\n",
        "          m.bias.data = m.bias.data.double()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "def make_data_loader(cluster_list, batch_sz, target_index, cluster_num):\n",
        "    cluster_dataset = ClusterDataset(cluster_list, target_index, cluster_num, batch_sz)\n",
        "    if cluster_dataset.__len__() == batch_sz:\n",
        "        cluster_loader = torch.utils.data.DataLoader(cluster_dataset, batch_size=batch_sz, shuffle=True, num_workers=4, drop_last=False)\n",
        "    else:\n",
        "        cluster_loader = torch.utils.data.DataLoader(cluster_dataset, batch_size=batch_sz, shuffle=True, num_workers=4, drop_last=True)\n",
        "    return cluster_loader\n",
        "\n",
        "\n",
        "def generate_loader_dict(total_list, unified_cluster_list, batch_sz, target_index, cluster_num):\n",
        "    loader_dict = {}\n",
        "    for cluster_id in total_list:\n",
        "        cluster_loader = make_data_loader([cluster_id], batch_sz, target_index, cluster_num)\n",
        "        loader_dict[cluster_id] = cluster_loader\n",
        "\n",
        "    for cluster_tuple in unified_cluster_list:\n",
        "        cluster_loader = make_data_loader(cluster_tuple, batch_sz, target_index, cluster_num)\n",
        "        for cluster_id in cluster_tuple:\n",
        "            loader_dict[cluster_id] = cluster_loader\n",
        "    return loader_dict\n",
        "\n",
        "\n",
        "\n",
        "def main(lr, batchsz, epochs, seed, lamb, alpha, cluster_num, target_index, modelname):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = MLP()\n",
        "    model.load_state_dict(torch.load('./AirbnbData/Chicago/checkpoint/{}'.format(modelname))['model'])\n",
        "    model.to(device)\n",
        "\n",
        "    valid_data = pd.read_csv('./NYC_Experiments/data/test-gridsize-{}.csv'.format(str(target_index)[5:]))[['label']]\n",
        "    train_tmp = pd.read_csv('./NYC_Experiments/data/train-gridsize-{}.csv'.format(str(target_index)[5:]))[['label']]\n",
        "    train_y = train_tmp.values\n",
        "    valid_data = np.squeeze((np.array(valid_data.values) - np.min(train_y))/(np.max(train_y)-np.min(train_y)))\n",
        "    model.eval()\n",
        "\n",
        "    test_dataset = TestDataset(target_index, cluster_num)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batchsz, shuffle=False, num_workers=4)\n",
        "\n",
        "    prediction_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, name) in enumerate(test_loader):\n",
        "              data = data.to(device)\n",
        "              scores = model(data).squeeze()\n",
        "              count = 0\n",
        "              for each_name in name:\n",
        "                  if len(name) > 1:\n",
        "                    prediction_list.append(scores[count].cpu().data.numpy())\n",
        "                  else:\n",
        "                    prediction_list.append(scores.cpu().data.numpy())\n",
        "                  count+=1\n",
        "\n",
        "    x, y = np.array(prediction_list).astype(float), valid_data.astype(float)\n",
        "\n",
        "    print(spearmanr(np.array(prediction_list), valid_data).statistic, pearsonr(x, y).statistic)\n",
        "\n",
        "\n",
        "for target_index in ['index002','index004','index006','index008','index01']:\n",
        "      dummy = pd.read_csv('./AirbnbData/Chicago/finch-train-gridsize-{}-c.csv'.format(str(target_index)[5:]))[['cluster_labels']]\n",
        "      cluster_num = len(list(set(list(train_data_class['cluster_labels'].values))))\n",
        "      print(target_index, cluster_num)\n",
        "      main(lr = 0.0002, batchsz=32, epochs=1000, seed=1567010775, lamb=30, alpha=4, cluster_num=cluster_num, target_index=target_index, modelname = 'finch-checkpoint_target_{}_c.pth'.format(target_index))"
      ],
      "metadata": {
        "id": "I7Ei8SPPScDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}