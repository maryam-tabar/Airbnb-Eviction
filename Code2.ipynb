{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Credit: Codes were adopted from https://github.com/Sungwon-Han/urban_score\n","\n","Sungwon Han, Donghyun Ahn, Sungwon Park, Jeasurk Yang, Susang Lee, Jihee Kim, Hyunjoo Yang, Sangyoon Park, and Meeyoung Cha. 2020. Learning to Score Economic Development from Satellite Imagery. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD '20). Association for Computing Machinery, New York, NY, USA, 2970â€“2979. https://doi.org/10.1145/3394486.3403347"],"metadata":{"id":"o_CPf_w7IMcI"}},{"cell_type":"code","source":[],"metadata":{"id":"6nIeQwVDIN4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import torch.nn.parallel\n","import torch.utils.data\n","import pickle\n","import time\n","import os\n","import faiss\n","import numpy as np\n","import torch\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","import torch.nn as nn\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn import preprocessing\n","\n","class GPSDataset(Dataset):\n","    def __init__(self, metadata, target_index):\n","        train_data = pd.read_csv(metadata)\n","        train_data.drop(['label', target_index], axis=1, inplace=True)\n","        X_train =  train_data.values\n","\n","        # impute missing values\n","        imputer = SimpleImputer(strategy='mean')\n","        imputer_model = imputer.fit(X_train)\n","        X_train = imputer_model.transform(X_train)\n","\n","        # min-max scaling\n","        min_max_scaler = preprocessing.MinMaxScaler()\n","        min_max_scaler_model = min_max_scaler.fit(X_train)\n","        self.metadata = min_max_scaler_model.transform(X_train)\n","\n","    def __len__(self):\n","        return len(self.metadata)\n","\n","    def __getitem__(self, idx):\n","        return self.metadata[idx], idx\n","\n","def run_kmeans(x, nmb_clusters, cnum, target_index):\n","    n_data, d = x.shape\n","\n","    # faiss implementation of k-means\n","    clus = faiss.Clustering(d, nmb_clusters)\n","\n","    # Change faiss seed at each k-means so that the randomly picked\n","    # initialization centroids do not correspond to the same feature ids\n","    # from an epoch to another.\n","    clus.seed = np.random.randint(1234)\n","\n","    clus.niter = 20\n","    # clus.min_points_per_centroid=1\n","    clus.max_points_per_centroid = 10000000\n","    res = faiss.StandardGpuResources()\n","    flat_config = faiss.GpuIndexFlatConfig()\n","    flat_config.useFloat16 = False\n","    flat_config.device = 0\n","    index = faiss.GpuIndexFlatL2(res, d, flat_config)\n","\n","    # perform the training\n","    clus.train(x, index)\n","    _, I = index.search(x, 1)\n","    # losses = faiss.vector_to_array(clus.obj)\n","    stats = clus.iteration_stats\n","    losses = np.array([\n","        stats.at(i).obj for i in range(stats.size())\n","    ])\n","    # print('k-means loss evolution: {0}'.format(losses))\n","\n","    faiss.write_index(faiss.index_gpu_to_cpu(index), \"./AirbnbData/Boston Airbnb 07:2023/kmeans-index-rawfeatures-target-{}-cnum{}.bin\".format(target_index, cnum))\n","    return [int(n[0]) for n in I], losses[-1]\n","\n","\n","def compute_features(dataloader, N, batch_size):\n","    # discard the label information in the dataloader\n","    for i, (inputs, _) in enumerate(dataloader):\n","        aux = np.array(inputs)\n","        if i == 0:\n","            features = np.zeros((N, aux.shape[1]), dtype='float32')\n","\n","        aux = aux.astype('float32')\n","        if i < len(dataloader) - 1:\n","            features[i * batch_size: (i + 1) * batch_size] = aux\n","        else:\n","            features[i * batch_size:] = aux\n","\n","    return features\n","\n","\n","class Kmeans(object):\n","    def __init__(self, k):\n","        self.k = k\n","\n","    def cluster(self, data, cnum, target_index):\n","      while True:\n","        end = time.time()\n","        # cluster the data\n","        I, loss = run_kmeans(data, self.k, cnum, target_index)\n","        self.images_lists = [[] for i in range(self.k)]\n","        label = []\n","        for i in range(len(data)):\n","            label.append(I[i])\n","            self.images_lists[I[i]].append(i)\n","\n","        if len(set(label)) <  cnum:\n","            print(set(label), '********** not complete **********')\n","        else:\n","            label = torch.tensor(label).cuda()\n","            return loss, label\n","\n","\n","def extract_data_cluster(cnum, target_index):\n","    clusterset = GPSDataset('./AirbnbData/Boston Airbnb 07:2023/train-gridsize-{}.csv'.format(str(target_index)[5:]), target_index)\n","    clusterloader = torch.utils.data.DataLoader(clusterset, batch_size=256, shuffle=False, num_workers=1)\n","    deepcluster = Kmeans(cnum)\n","    features = compute_features(clusterloader, len(clusterset), 256)\n","    clustering_loss, p_label = deepcluster.cluster(features, cnum, target_index)\n","    labels = p_label.tolist()\n","    return labels\n","\n","\n","def main():\n","    for target_index in ['index002','index004','index006','index008','index01']:\n","        for cnum in range(2,16):\n","            print(target_index, cnum)\n","            train_data = pd.read_csv('./AirbnbData/Boston Airbnb 07:2023/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n","            cluster_labels = extract_data_cluster(cnum, target_index)\n","            result = pd.DataFrame({\"y_train\":list(train_data['label'].values), \"cluster_labels\":cluster_labels})\n","            result.to_csv('./AirbnbData/Boston Airbnb 07:2023/train-gridsize-{}-c{}.csv'.format(str(target_index)[5:], cnum))\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"],"metadata":{"id":"RhqbyeokLvu1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from scipy import stats\n","from collections import defaultdict\n","from functools import cmp_to_key\n","from scipy.optimize import nnls\n","\n","\n","class Graph:\n","    def __init__(self,vertices):\n","        self.V= vertices\n","        self.graph = defaultdict(list)\n","\n","    def addEdge(self,u,v):\n","        self.graph[u].append(v)\n","\n","    def printPathsFunc(self, u, d, visited, path, current_path_list):\n","        visited[u]= True\n","        path.append(u)\n","\n","        if u == d:\n","            path_copy = path[:]\n","            current_path_list.append(path_copy)\n","        else:\n","            for i in self.graph[u]:\n","                if visited[i]==False:\n","                    self.printPathsFunc(i, d, visited, path, current_path_list)\n","\n","        path.pop()\n","        visited[u]= False\n","        return current_path_list\n","\n","\n","    def printPaths(self, s, d):\n","        total_results = []\n","        for start in s:\n","            for dest in d:\n","                path = []\n","                visited =[False]*(self.V)\n","                current_path_list = []\n","                current_path_results = self.printPathsFunc(start, dest, visited, path, current_path_list)\n","                if len(current_path_results) != 0:\n","                    total_results.extend(current_path_results)\n","        return total_results\n","\n","\n","def graph_process(config_path):\n","    cluster_unify = []\n","    partial_order = []\n","    start_candidates = []\n","    end_candidates = []\n","\n","    f = open(config_path, 'r')\n","    while True:\n","        line = f.readline()\n","        if '=' in line:\n","            unify = list(map(int, line.split('=')))\n","            cluster_unify.append(unify)\n","        elif '<' in line:\n","            order = list(map(int, line.split('<')))\n","            partial_order.append(order)\n","            start_candidates.append(order[0])\n","            end_candidates.append(order[1])\n","\n","        if not line: break\n","    f.close()\n","\n","    start = []\n","    end = []\n","    for element in start_candidates:\n","        if element in end_candidates:\n","            continue\n","        start.append(element)\n","\n","    for element in end_candidates:\n","        if element in start_candidates:\n","            continue\n","        end.append(element)\n","\n","    start = list(set(start))\n","    end = list(set(end))\n","    return start, end, partial_order, cluster_unify\n","\n","\n","\n","def generate_graph(partial_order_list, vertex_num):\n","    cluster_graph = Graph(vertex_num)\n","    for pair in partial_order_list:\n","        cluster_graph.addEdge(pair[0], pair[1])\n","    return cluster_graph\n","\n","\n","def save_graph_config(ordered_list, name):\n","    f = open(\"./AirbnbData/Boston Airbnb 07:2023/{}\".format(name), 'w+')\n","\n","    for i in range(len(ordered_list) - 1):\n","        f.write('{}<{}\\n'.format(ordered_list[i+1][0], ordered_list[i][0]))\n","\n","    for orders in ordered_list:\n","        if len(orders) >= 2:\n","            f.write(str(orders[0]))\n","            for element in orders[1:]:\n","                f.write('={}'.format(element))\n","            f.write('\\n')\n","    f.close()\n","\n","\n","\n","def graph_inference_census(df, cluster_num, file_path):\n","    def numeric_compare(x, y):\n","        pop_list1 = result_list[x]\n","        pop_list2 = result_list[y]\n","        tTestResult = stats.ttest_ind(pop_list1, pop_list2)\n","        if (tTestResult.pvalue < 0.05) and (np.mean(pop_list1) < np.mean(pop_list2)):\n","            return 1\n","        elif (tTestResult.pvalue < 0.05) and (np.mean(pop_list1) >= np.mean(pop_list2)):\n","            return -1\n","        else:\n","            return 0\n","\n","    result_list = []\n","\n","    for i in range(cluster_num):\n","        result_list.append([])\n","\n","    for _ in range(100):\n","        msk = np.random.rand(len(df)) < 0.5\n","        df_train = df[msk][(['label{}'.format(i) for i in range(cluster_num)]+['CensusPop'])]\n","        df_test = df[~msk][(['label{}'.format(i) for i in range(cluster_num)]+['CensusPop'])]\n","\n","        # selected_hist_train = hist.loc[df_train['Directory'] - 1]\n","        # selected_hist_test = hist.loc[df_test['Directory'] - 1]\n","\n","        train_X = df_train[['label{}'.format(i) for i in range(cluster_num)]].values\n","        train_y = df_train[\"CensusPop\"].values\n","        test_X = df_train[['label{}'.format(i) for i in range(cluster_num)]].values\n","        test_y = df_test[\"CensusPop\"].values\n","        result = nnls(train_X, train_y, 100)[0]\n","        for i in range(len(result)):\n","            result_list[i].append(result[i])\n","\n","    sorted_list = sorted(range(cluster_num ), key=cmp_to_key(numeric_compare))\n","    ordered_list = []\n","    ordered_list.append([sorted_list[0]])\n","    curr = 0\n","    for i in range(len(sorted_list) - 1):\n","        if numeric_compare(sorted_list[i], sorted_list[i+1]) == 0:\n","            ordered_list[curr].append(sorted_list[i+1])\n","        else:\n","            curr += 1\n","            ordered_list.append([sorted_list[i+1]])\n","\n","    # ordered_list.append([cluster_num])\n","    print(ordered_list)\n","    save_graph_config(ordered_list, file_path)\n","    return './graph_data/{}'.format(file_path)\n","\n","for target_index in ['index002','index004','index006','index008','index01']:\n","    for cnum in range(2,16):\n","        df = pd.read_csv('./AirbnbData/Boston Airbnb 07:2023/NYC-census-guided-targetindex{}-c{}.csv'.format(str(target_index)[5:], cnum))\n","        graph_inference_census(df, cnum, \"graph-config-target{}-c{}.txt\".format(target_index,cnum))\n"],"metadata":{"id":"MF1Yz8-tSfrv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from scipy import stats\n","from collections import defaultdict\n","from functools import cmp_to_key\n","from scipy.optimize import nnls\n","\n","\n","class Graph:\n","    def __init__(self,vertices):\n","        self.V= vertices\n","        self.graph = defaultdict(list)\n","\n","    def addEdge(self,u,v):\n","        self.graph[u].append(v)\n","\n","    def printPathsFunc(self, u, d, visited, path, current_path_list):\n","        visited[u]= True\n","        path.append(u)\n","\n","        if u == d:\n","            path_copy = path[:]\n","            current_path_list.append(path_copy)\n","        else:\n","            for i in self.graph[u]:\n","                if visited[i]==False:\n","                    self.printPathsFunc(i, d, visited, path, current_path_list)\n","\n","        path.pop()\n","        visited[u]= False\n","        return current_path_list\n","\n","\n","    def printPaths(self, s, d):\n","        total_results = []\n","        for start in s:\n","            for dest in d:\n","                path = []\n","                visited =[False]*(self.V)\n","                current_path_list = []\n","                current_path_results = self.printPathsFunc(start, dest, visited, path, current_path_list)\n","                if len(current_path_results) != 0:\n","                    total_results.extend(current_path_results)\n","        return total_results\n","\n","\n","def graph_process(config_path):\n","    cluster_unify = []\n","    partial_order = []\n","    start_candidates = []\n","    end_candidates = []\n","\n","    f = open(config_path, 'r')\n","    while True:\n","        line = f.readline()\n","        if '=' in line:\n","            unify = list(map(int, line.split('=')))\n","            cluster_unify.append(unify)\n","        elif '<' in line:\n","            order = list(map(int, line.split('<')))\n","            partial_order.append(order)\n","            start_candidates.append(order[0])\n","            end_candidates.append(order[1])\n","\n","        if not line: break\n","    f.close()\n","\n","    start = []\n","    end = []\n","    for element in start_candidates:\n","        if element in end_candidates:\n","            continue\n","        start.append(element)\n","\n","    for element in end_candidates:\n","        if element in start_candidates:\n","            continue\n","        end.append(element)\n","\n","    start = list(set(start))\n","    end = list(set(end))\n","    return start, end, partial_order, cluster_unify\n","\n","\n","\n","def generate_graph(partial_order_list, vertex_num):\n","    cluster_graph = Graph(vertex_num)\n","    for pair in partial_order_list:\n","        cluster_graph.addEdge(pair[0], pair[1])\n","    return cluster_graph\n","\n","\n","def save_graph_config(ordered_list, name):\n","    f = open(\"./AirbnbData/Boston Airbnb 07:2023/{}\".format(name), 'w+')\n","\n","    for i in range(len(ordered_list) - 1):\n","        f.write('{}<{}\\n'.format(ordered_list[i+1][0], ordered_list[i][0]))\n","\n","    for orders in ordered_list:\n","        if len(orders) >= 2:\n","            f.write(str(orders[0]))\n","            for element in orders[1:]:\n","                f.write('={}'.format(element))\n","            f.write('\\n')\n","    f.close()\n","\n","\n","\n","def graph_inference_census(df, cluster_num, file_path):\n","    def numeric_compare(x, y):\n","        pop_list1 = result_list[x]\n","        pop_list2 = result_list[y]\n","        tTestResult = stats.ttest_ind(pop_list1, pop_list2)\n","        if (tTestResult.pvalue < 0.05) and (np.mean(pop_list1) < np.mean(pop_list2)):\n","            return 1\n","        elif (tTestResult.pvalue < 0.05) and (np.mean(pop_list1) >= np.mean(pop_list2)):\n","            return -1\n","        else:\n","            return 0\n","\n","    result_list = []\n","\n","    for i in range(cluster_num):\n","        result_list.append([])\n","\n","    for _ in range(100):\n","        msk = np.random.rand(len(df)) < 0.5\n","        df_train = df[msk][(['label{}'.format(i) for i in range(cluster_num)]+['CensusPop'])]\n","        df_test = df[~msk][(['label{}'.format(i) for i in range(cluster_num)]+['CensusPop'])]\n","\n","        # selected_hist_train = hist.loc[df_train['Directory'] - 1]\n","        # selected_hist_test = hist.loc[df_test['Directory'] - 1]\n","\n","        train_X = df_train[['label{}'.format(i) for i in range(cluster_num)]].values\n","        train_y = df_train[\"CensusPop\"].values\n","        test_X = df_train[['label{}'.format(i) for i in range(cluster_num)]].values\n","        test_y = df_test[\"CensusPop\"].values\n","        result = nnls(train_X, train_y, 100)[0]\n","        for i in range(len(result)):\n","            result_list[i].append(result[i])\n","\n","    sorted_list = sorted(range(cluster_num ), key=cmp_to_key(numeric_compare))\n","    ordered_list = []\n","    ordered_list.append([sorted_list[0]])\n","    curr = 0\n","    for i in range(len(sorted_list) - 1):\n","        if numeric_compare(sorted_list[i], sorted_list[i+1]) == 0:\n","            ordered_list[curr].append(sorted_list[i+1])\n","        else:\n","            curr += 1\n","            ordered_list.append([sorted_list[i+1]])\n","\n","    # ordered_list.append([cluster_num])\n","    save_graph_config(ordered_list, file_path)\n","    return './{}'.format(file_path)\n"],"metadata":{"id":"VGnlhR9rjOqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from torch.utils.data import Dataset\n","import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import warnings\n","from scipy.stats import spearmanr, pearsonr\n","import pickle\n","from datetime import datetime\n","from numpy import nanmin, nanmax, nanmean, nanvar, nanmedian\n","from sklearn.impute import SimpleImputer\n","from scipy.stats import spearmanr, pearsonr\n","from sklearn import preprocessing\n","\n","\n","class TestDataset(Dataset):\n","    def __init__(self, target_index, cnum):\n","        test_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))\n","        train_data = pd.read_csv('./AirbnbData/Boston Airbnb 07:2023/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n","        test_data.drop(['label', target_index], axis=1, inplace=True)\n","        train_data.drop(['label', target_index], axis=1, inplace=True)\n","\n","        X_train =  train_data.values\n","        imputer = SimpleImputer(strategy='mean')\n","        imputer_model = imputer.fit(X_train)\n","        X_train = imputer_model.transform(X_train)\n","\n","        X_test = imputer_model.transform(test_data.values)\n","\n","        # min-max scaling\n","        min_max_scaler_model = (preprocessing.MinMaxScaler()).fit(X_train)\n","        X_train = min_max_scaler_model.transform(X_train)\n","        X_test = min_max_scaler_model.transform(X_test)\n","\n","        self.file_list = X_test\n","\n","    def __len__(self):\n","        return (self.file_list).shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.file_list[idx], idx\n","\n","\n","from sklearn.utils import resample\n","\n","class ClusterDataset(Dataset):\n","    def __init__(self, cluster_list, target_index, cnum, batch_sz):\n","        self.data_list = None\n","        train_data_class = pd.read_csv('./AirbnbData/Boston Airbnb 07:2023/train-gridsize-{}-c{}.csv'.format(str(target_index)[5:], cnum))[['cluster_labels']]\n","        train_data_class['cluster_labels'] = train_data_class['cluster_labels'].map(lambda x: int(x))\n","        train_data = pd.read_csv('./AirbnbData/Boston Airbnb 07:2023/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n","        train_data.drop(['label', target_index], axis=1, inplace=True)\n","\n","        X_train =  train_data.values\n","        imputer = SimpleImputer(strategy='mean')\n","        imputer_model = imputer.fit(X_train)\n","        X_train = imputer_model.transform(X_train)\n","        # min-max scaling\n","        min_max_scaler_model = (preprocessing.MinMaxScaler()).fit(X_train)\n","        X_train = min_max_scaler_model.transform(X_train)\n","        train_data_v2 = pd.DataFrame(X_train, columns = train_data.columns)\n","\n","        for cluster_num in cluster_list:\n","            tmp =train_data_v2[train_data_class['cluster_labels']==int(cluster_num)]\n","            if self.data_list is None:\n","                self.data_list = tmp.values\n","            else:\n","                self.data_list = np.concatenate([self.data_list, tmp.values])\n","\n","        if (self.data_list.shape[0]<batch_sz) and (self.data_list.shape[0] > 0):\n","              self.data_list = (resample(pd.DataFrame(self.data_list), n_samples=batch_sz,replace=True)).values\n","\n","    def __len__(self):\n","        return (self.data_list).shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.data_list[idx]\n"],"metadata":{"id":"4ORQozN47vKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","import pandas as pd\n","from torch.utils.data import DataLoader\n","import argparse\n","from itertools import permutations\n","\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","class MLP(nn.Module):\n","  '''\n","    Multilayer Perceptron.\n","  '''\n","  def __init__(self):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","      nn.Linear(181, 128),\n","      nn.ReLU(),\n","      nn.Linear(128, 64),\n","      nn.ReLU(),\n","      nn.Linear(64, 32),\n","      nn.ReLU(),\n","      nn.Linear(32, 16),\n","      nn.ReLU(),\n","      nn.Linear(16, 8),\n","      nn.ReLU(),\n","      nn.Linear(8, 1),\n","      nn.ReLU()\n","    )\n","    self.layers.cuda()\n","    for m in self.layers.modules():\n","      if isinstance(m, nn.Linear):\n","          m.weight.data = m.weight.data.double()\n","          m.bias.data = m.bias.data.double()\n","\n","\n","  def forward(self, x):\n","    return self.layers(x)\n","\n","\n","def make_data_loader(cluster_list, batch_sz, target_index, cluster_num):\n","    cluster_dataset = ClusterDataset(cluster_list, target_index, cluster_num, batch_sz)\n","    if cluster_dataset.__len__() == batch_sz:\n","        cluster_loader = torch.utils.data.DataLoader(cluster_dataset, batch_size=batch_sz, shuffle=True, num_workers=4, drop_last=False)\n","    else:\n","        cluster_loader = torch.utils.data.DataLoader(cluster_dataset, batch_size=batch_sz, shuffle=True, num_workers=4, drop_last=True)\n","    return cluster_loader\n","\n","\n","def generate_loader_dict(total_list, unified_cluster_list, batch_sz, target_index, cluster_num):\n","    loader_dict = {}\n","    for cluster_id in total_list:\n","        cluster_loader = make_data_loader([cluster_id], batch_sz, target_index, cluster_num)\n","        loader_dict[cluster_id] = cluster_loader\n","\n","    for cluster_tuple in unified_cluster_list:\n","        cluster_loader = make_data_loader(cluster_tuple, batch_sz, target_index, cluster_num)\n","        for cluster_id in cluster_tuple:\n","            loader_dict[cluster_id] = cluster_loader\n","    return loader_dict\n","\n","\n","def train(epoch, model, optimizer, loader_list, cluster_path_list, device, lr, batchsz, seed, lamb, alpha):\n","    model.train()\n","    train_loss = AverageMeter()\n","    reg_loss = AverageMeter()\n","\n","    # For each cluster route\n","    path_idx = 0\n","    avg_loss = 0\n","    count = 0\n","    for cluster_path in cluster_path_list:\n","        path_idx += 1\n","        dataloaders = []\n","        for cluster_id in cluster_path:\n","            dataloaders.append(loader_list[cluster_id])\n","\n","        for batch_idx, data in enumerate(zip(*dataloaders)):\n","            cluster_num = len(data)\n","            data_zip = torch.cat(data, 0).to(device)\n","\n","            # Generating Score\n","            scores = model(data_zip).squeeze()\n","            scores = torch.clamp(scores, min=0, max=1)\n","            score_list = torch.split(scores, batchsz, dim = 0)\n","\n","            # Standard deviation as a loss\n","            loss_var = torch.zeros(1).to(device)\n","            for score in score_list:\n","                loss_var += score.var()\n","            loss_var /= len(score_list)\n","\n","            # Differentiable Ranking with sigmoid function\n","            rank_matrix = torch.zeros((batchsz, cluster_num, cluster_num)).to(device)\n","            for itertuple in list(permutations(range(cluster_num), 2)):\n","                score1 = score_list[itertuple[0]]\n","                score2 = score_list[itertuple[1]]\n","                diff = lamb * (score2 - score1)\n","                results = torch.sigmoid(diff)\n","                rank_matrix[:, itertuple[0], itertuple[1]] = results\n","                rank_matrix[:, itertuple[1], itertuple[0]] = 1 - results\n","\n","            rank_predicts = rank_matrix.sum(1)\n","            temp = torch.Tensor(range(cluster_num))\n","            target_rank = temp.unsqueeze(0).repeat(batchsz, 1).to(device)\n","\n","            # Equivalent to spearman rank correlation loss\n","            loss_train = ((rank_predicts - target_rank)**2).mean()\n","            loss = loss_train + loss_var * alpha\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss.update(loss_train.item(), batchsz)\n","            reg_loss.update(loss_var.item(), batchsz)\n","            avg_loss += loss.item()\n","            count += 1\n","\n","\n","            # Print status\n","            if batch_idx % 10 == 0:\n","                print('Epoch: [{epoch}][{path_idx}][{elps_iters}] '\n","                      'Train loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '\n","                      'Reg loss: {reg_loss.val:.4f} ({reg_loss.avg:.4f})'.format(\n","                          epoch=epoch, path_idx=path_idx, elps_iters=batch_idx, train_loss=train_loss, reg_loss=reg_loss))\n","\n","    return avg_loss / count\n","\n","\n","def main(lr, batchsz, epochs, seed, lamb, alpha, cluster_num, target_index, modelname):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","\n","    graph_config = \"./AirbnbData/Boston Airbnb 07:2023/graph-config-target{}-c{}.txt\".format(target_index,cluster_num)\n","    start, end, partial_order, cluster_unify = graph_process(graph_config)\n","\n","    loader_list = generate_loader_dict(range(cluster_num), cluster_unify, batchsz, target_index, cluster_num)\n","    cluster_graph = generate_graph(partial_order, cluster_num)\n","    cluster_path_list = cluster_graph.printPaths(start, end)\n","    print(\"Cluster_path: \", cluster_path_list)\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model = MLP()\n","    model.to(device)\n","\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n","    valid_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))[['label']]\n","\n","    best_spearman_corr = -2\n","    patience = 0\n","    for epoch in range(epochs):\n","            loss = train(epoch, model, optimizer, loader_list, cluster_path_list, device, lr, batchsz, seed, lamb, alpha)\n","\n","            model.eval()\n","\n","            test_dataset = TestDataset(target_index, cluster_num)\n","            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batchsz, shuffle=False, num_workers=4)\n","\n","            prediction_list = []\n","            with torch.no_grad():\n","                for batch_idx, (data, name) in enumerate(test_loader):\n","                    data = data.to(device)\n","                    scores = model(data).squeeze()\n","                    count = 0\n","                    for each_name in name:\n","                        prediction_list.append(scores[count].cpu().data.numpy())\n","                        count += 1\n","\n","            spearman_corr = spearmanr(np.array(prediction_list), valid_data.values).statistic\n","            print(spearman_corr)\n","\n","            if best_spearman_corr < spearman_corr:\n","                    patience = 0\n","                    print(\"state saving...\")\n","                    state = {\n","                        'model': model.state_dict(),\n","                        'loss': loss\n","                    }\n","                    if not os.path.isdir('./AirbnbData/Boston Airbnb 07:2023/checkpoint'):\n","                        os.mkdir('./AirbnbData/Boston Airbnb 07:2023/checkpoint')\n","                    torch.save(state, './AirbnbData/Boston Airbnb 07:2023/checkpoint/{}'.format(modelname))\n","                    best_spearman_corr = spearman_corr\n","            else:\n","                    patience += 1\n","                    if patience > 10:\n","                      break\n","\n","for target_index in ['index002','index004','index006','index008','index01']:\n","  for cluster_num in range(2,16):\n","      print(target_index, cluster_num)\n","      main(lr = 0.0002, batchsz=32, epochs=1000, seed=1567010775, lamb=30, alpha=4, cluster_num=cluster_num, target_index=target_index, modelname = 'checkpoint_target_{}_c{}.pth'.format(target_index, cluster_num))\n"],"metadata":{"id":"lKDEpxgBdv8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jaGCVibVILTg"},"execution_count":null,"outputs":[]}]}