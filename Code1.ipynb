{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gBPZS8IeFBsz"},"outputs":[],"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","from numpy import nanmin, nanmax, nanmean, nanvar, nanmedian\n","from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from scipy.stats import spearmanr, pearsonr\n","from sklearn import preprocessing\n","from sklearn.metrics import mean_squared_error\n","\n","\n","for target_index in ['index002','index004','index006','index008','index01']:\n","    train_data = pd.read_csv('./NYC_Experiments/data/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    val_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    test_data = pd.read_csv('./NYC_Experiments/data/test-gridsize-{}.csv'.format(str(target_index)[5:]))\n","\n","    y_train, y_val, y_test = train_data['label'].values, val_data['label'].values, test_data['label'].values\n","\n","    train_data.drop(['label', target_index], axis=1, inplace=True)\n","    val_data.drop(['label', target_index], axis=1, inplace=True)\n","    test_data.drop(['label', target_index], axis=1, inplace=True)\n","\n","    X_train, X_val, X_test =  train_data.values, val_data.values, test_data.values\n","\n","\n","    # impute missing values\n","    imputer = SimpleImputer(strategy='mean')\n","    imputer_model = imputer.fit(X_train)\n","    X_train, X_val, X_test = imputer_model.transform(X_train), imputer_model.transform(X_val), imputer_model.transform(X_test)\n","\n","    # min-max scaling\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    min_max_scaler_model = min_max_scaler.fit(X_train)\n","    X_train, X_val, X_test = min_max_scaler_model.transform(X_train), min_max_scaler_model.transform(X_val), min_max_scaler_model.transform(X_test)\n","\n","    y_min, y_max = np.min(y_train), np.max(y_train)\n","    y_train, y_val, y_test = ((y_train-y_min)/(y_max-y_min)), ((y_val-y_min)/(y_max-y_min)), ((y_test-y_min)/(y_max-y_min))\n","\n","\n","    # Random Forest model\n","    best_model, best_acc = None, np.inf\n","    for max_depth_i in [4,8,16, None]:\n","      regr = RandomForestRegressor(max_depth=max_depth_i)\n","      regr.fit(X_train, y_train)\n","      res = mean_squared_error(regr.predict(X_val), y_val)\n","      if res < best_acc:\n","          best_acc = res\n","          best_model = regr\n","\n","    y_test_predicted = best_model.predict(X_test)\n","    print('Random Forest: ',target_index, '---', spearmanr(y_test_predicted, y_test).statistic, '---', pearsonr(y_test_predicted, y_test).statistic)\n","\n","    # save the model to disk\n","    pickle.dump(best_model, open('./NYC_Experiments/models/rf_model_{}.sav'.format(target_index), 'wb'))\n","    (pd.DataFrame({'y_pred':y_test_predicted, 'y_true':y_test})).to_csv('./NYC_Experiments/results/rf_model_{}.csv'.format(target_index), index=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KvRfGdfkgMo"},"outputs":[],"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","from numpy import nanmin, nanmax, nanmean, nanvar, nanmedian\n","from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from scipy.stats import spearmanr, pearsonr\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from sklearn.metrics import mean_squared_error\n","\n","for target_index in ['index002','index004','index006','index008','index01']:\n","    train_data = pd.read_csv('./NYC_Experiments/data/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    val_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    test_data = pd.read_csv('./NYC_Experiments/data/test-gridsize-{}.csv'.format(str(target_index)[5:]))\n","\n","    y_train, y_val, y_test = train_data['label'].values, val_data['label'].values, test_data['label'].values\n","\n","    train_data.drop(['label', target_index], axis=1, inplace=True)\n","    val_data.drop(['label', target_index], axis=1, inplace=True)\n","    test_data.drop(['label', target_index], axis=1, inplace=True)\n","\n","    X_train, X_val, X_test =  train_data.values, val_data.values, test_data.values\n","\n","\n","    # impute missing values\n","    imputer = SimpleImputer(strategy='mean')\n","    imputer_model = imputer.fit(X_train)\n","    X_train, X_val, X_test = imputer_model.transform(X_train), imputer_model.transform(X_val), imputer_model.transform(X_test)\n","\n","    # min-max scaling\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    min_max_scaler_model = min_max_scaler.fit(X_train)\n","    X_train, X_val, X_test = min_max_scaler_model.transform(X_train), min_max_scaler_model.transform(X_val), min_max_scaler_model.transform(X_test)\n","\n","    y_min, y_max = np.min(y_train), np.max(y_train)\n","    y_train, y_val, y_test = ((y_train-y_min)/(y_max-y_min)), ((y_val-y_min)/(y_max-y_min)), ((y_test-y_min)/(y_max-y_min))\n","\n","\n","    # Random Forest model\n","    best_model, best_acc = None, np.inf\n","    for max_depth_i in [4,8,16, None]:\n","      regr = xgb.XGBRegressor(max_depth=max_depth_i)\n","      regr.fit(X_train, y_train)\n","      res = mean_squared_error(regr.predict(X_val), y_val)\n","      if res < best_acc:\n","          best_acc = res\n","          best_model = regr\n","\n","    y_test_predicted = best_model.predict(X_test)\n","    print('XGBoost: ',target_index, '---', spearmanr(y_test_predicted, y_test).statistic, '---', pearsonr(y_test_predicted, y_test).statistic)\n","\n","    # save the model to disk\n","    pickle.dump(best_model, open('./NYC_Experiments/models/xgb_model_{}.sav'.format(target_index), 'wb'))\n","    (pd.DataFrame({'y_pred':y_test_predicted, 'y_true':y_test})).to_csv('./NYC_Experiments/results/xgb_model_{}.csv'.format(target_index), index=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJ8po06Bp73b"},"outputs":[],"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","from numpy import nanmin, nanmax, nanmean, nanvar, nanmedian\n","from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from scipy.stats import spearmanr, pearsonr\n","from sklearn import preprocessing\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_error\n","\n","for target_index in ['index002','index004','index006','index008','index01']:\n","    train_data = pd.read_csv('./NYC_Experiments/data/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    val_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    test_data = pd.read_csv('./NYC_Experiments/data/test-gridsize-{}.csv'.format(str(target_index)[5:]))\n","\n","    y_train, y_val, y_test = train_data['label'].values, val_data['label'].values, test_data['label'].values\n","\n","    train_data.drop(['label', target_index], axis=1, inplace=True)\n","    val_data.drop(['label', target_index], axis=1, inplace=True)\n","    test_data.drop(['label', target_index], axis=1, inplace=True)\n","\n","    X_train, X_val, X_test =  train_data.values, val_data.values, test_data.values\n","\n","\n","    # impute missing values\n","    imputer = SimpleImputer(strategy='mean')\n","    imputer_model = imputer.fit(X_train)\n","    X_train, X_val, X_test = imputer_model.transform(X_train), imputer_model.transform(X_val), imputer_model.transform(X_test)\n","\n","    # min-max scaling\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    min_max_scaler_model = min_max_scaler.fit(X_train)\n","    X_train, X_val, X_test = min_max_scaler_model.transform(X_train), min_max_scaler_model.transform(X_val), min_max_scaler_model.transform(X_test)\n","\n","    y_min, y_max = np.min(y_train), np.max(y_train)\n","    y_train, y_val, y_test = ((y_train-y_min)/(y_max-y_min)), ((y_val-y_min)/(y_max-y_min)), ((y_test-y_min)/(y_max-y_min))\n","\n","\n","    # Random Forest model\n","    best_model, best_acc = None, np.inf\n","    for my_kernel in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]:\n","      regr = SVR(kernel=my_kernel)\n","      regr.fit(X_train, y_train)\n","      res = mean_squared_error(regr.predict(X_val), y_val)\n","      if res < best_acc:\n","          best_acc = res\n","          best_model = regr\n","\n","    y_test_predicted = best_model.predict(X_test)\n","    print('SVR: ',target_index, '---', spearmanr(y_test_predicted, y_test).statistic, '---', pearsonr(y_test_predicted, y_test).statistic)\n","\n","    # save the model to disk\n","    pickle.dump(best_model, open('./NYC_Experiments/models/SVR_model_{}.sav'.format(target_index), 'wb'))\n","    (pd.DataFrame({'y_pred':y_test_predicted, 'y_true':y_test})).to_csv('./NYC_Experiments/results/SVR_model_{}.csv'.format(target_index), index=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJzZq5HrtW1X"},"outputs":[],"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","from numpy import nanmin, nanmax, nanmean, nanvar, nanmedian\n","from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from scipy.stats import spearmanr, pearsonr\n","from sklearn import preprocessing\n","\n","!pip install pytorch-tabnet\n","from pytorch_tabnet.tab_model import TabNetRegressor\n","\n","\n","for target_index in ['index002','index004','index006','index008','index01']:\n","    train_data = pd.read_csv('./NYC_Experiments/data/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    val_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    test_data = pd.read_csv('./NYC_Experiments/data/test-gridsize-{}.csv'.format(str(target_index)[5:]))\n","\n","    y_train, y_val, y_test = train_data['label'].values, val_data['label'].values, test_data['label'].values\n","\n","    train_data.drop(['label', target_index], axis=1, inplace=True)\n","    val_data.drop(['label', target_index], axis=1, inplace=True)\n","    test_data.drop(['label', target_index], axis=1, inplace=True)\n","\n","    X_train, X_val, X_test =  train_data.values, val_data.values, test_data.values\n","\n","\n","    # impute missing values\n","    imputer = SimpleImputer(strategy='mean')\n","    imputer_model = imputer.fit(X_train)\n","    X_train, X_val, X_test = imputer_model.transform(X_train), imputer_model.transform(X_val), imputer_model.transform(X_test)\n","\n","    # min-max scaling\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    min_max_scaler_model = min_max_scaler.fit(X_train)\n","    X_train, X_val, X_test = min_max_scaler_model.transform(X_train), min_max_scaler_model.transform(X_val), min_max_scaler_model.transform(X_test)\n","\n","    y_min, y_max = np.min(y_train), np.max(y_train)\n","    y_train, y_val, y_test = ((y_train-y_min)/(y_max-y_min)), ((y_val-y_min)/(y_max-y_min)), ((y_test-y_min)/(y_max-y_min))\n","\n","\n","    my_regressor = TabNetRegressor()\n","    my_regressor.fit(X_train, np.reshape(y_train,(-1,1)), eval_set=[(X_val, np.reshape(y_val,(-1,1)))],max_epochs = 200, batch_size=32)\n","    y_test_predicted = my_regressor.predict(X_test)\n","    y_test_predicted = np.reshape(y_test_predicted, (y_test_predicted.shape[0],))\n","    print('TabNet: ',target_index, '---', spearmanr(y_test_predicted, y_test).statistic, '---', pearsonr(y_test_predicted, y_test).statistic)\n","    my_regressor.save_model('./NYC_Experiments/models/Tabnet_model_{}'.format(target_index))\n","    (pd.DataFrame({'y_pred':y_test_predicted, 'y_true':y_test})).to_csv('./NYC_Experiments/results/Tabnet_model_{}.csv'.format(target_index), index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"scM7vQFW5n3J"},"outputs":[],"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","from numpy import nanmin, nanmax, nanmean, nanvar, nanmedian\n","from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from scipy.stats import spearmanr, pearsonr\n","from sklearn import preprocessing\n","import xgboost as xgb\n","!pip install catboost\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","for target_index in ['index002','index004','index006','index008','index01']:\n","    train_data = pd.read_csv('./NYC_Experiments/data/train-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    val_data = pd.read_csv('./NYC_Experiments/data/val-gridsize-{}.csv'.format(str(target_index)[5:]))\n","    test_data = pd.read_csv('./NYC_Experiments/data/test-gridsize-{}.csv'.format(str(target_index)[5:]))\n","\n","    y_train, y_val, y_test = train_data['label'].values, val_data['label'].values, test_data['label'].values\n","\n","    train_data.drop(['label', target_index], axis=1, inplace=True)\n","    val_data.drop(['label', target_index], axis=1, inplace=True)\n","    test_data.drop(['label', target_index], axis=1, inplace=True)\n","\n","    X_train, X_val, X_test =  train_data.values, val_data.values, test_data.values\n","\n","\n","    # impute missing values\n","    imputer = SimpleImputer(strategy='mean')\n","    imputer_model = imputer.fit(X_train)\n","    X_train, X_val, X_test = imputer_model.transform(X_train), imputer_model.transform(X_val), imputer_model.transform(X_test)\n","\n","    # min-max scaling\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    min_max_scaler_model = min_max_scaler.fit(X_train)\n","    X_train, X_val, X_test = min_max_scaler_model.transform(X_train), min_max_scaler_model.transform(X_val), min_max_scaler_model.transform(X_test)\n","\n","    y_min, y_max = np.min(y_train), np.max(y_train)\n","    y_train, y_val, y_test = ((y_train-y_min)/(y_max-y_min)), ((y_val-y_min)/(y_max-y_min)), ((y_test-y_min)/(y_max-y_min))\n","\n","\n","    # CatBoostRegressor model\n","    best_model, best_acc = None, np.inf\n","    for max_depth_i in [4,8,16]:\n","      regr = CatBoostRegressor(max_depth=max_depth_i)\n","      regr.fit(X_train, y_train)\n","      res = mean_squared_error(regr.predict(X_val), y_val)\n","      if res < best_acc:\n","          best_acc = res\n","          best_model = regr\n","\n","    y_test_predicted = best_model.predict(X_test)\n","    print('CatBoostRegressor: ',target_index, '---', spearmanr(y_test_predicted, y_test).statistic, '---', pearsonr(y_test_predicted, y_test).statistic)\n","\n","    (pd.DataFrame({'y_pred':y_test_predicted, 'y_true':y_test})).to_csv('./NYC_Experiments/results/CatBoostRegressor_model_{}.csv'.format(target_index), index=False)\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}